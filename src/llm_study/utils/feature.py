""" load pt files from a directory as numpy ndarrays which is generated by llama3_hook
Name of the pt files should be like: layer{layer_idx}_step{step_idx}_{tensor_name}_{RoPE}.pt
"""

import numpy as np
import torch
import os
from typing import Literal, Union
from pathlib import Path

def load(path: str | os.PathLike, layer: int, step: int, type: Literal["query", "key", "value", "attn_weights"], rope: bool = False) -> np.ndarray:
    """ load pt files from a directory as numpy ndarrays """
    # 组装文件路径，并读取 .pt 文件为 numpy ndarray，考虑 RoPE 后缀
    filename = f"layer{layer}_step{step}_{type}"
    if rope:
        filename += "_RoPE"
    filename += ".pt"
    file_path = Path(path) / filename
    tensor = torch.load(file_path, map_location="cpu")
    arr = tensor.cpu().numpy()
    return arr

def stat(path: str | os.PathLike) -> dict:
    """
    扫描路径下的文件，统计step总数和layer总数，统计不重复的类型集合
    """
    import re

    file_pat = re.compile(r"layer(\d+)_step(\d+)_([a-zA-Z_]+)\.pt$")
    steps = 0
    layers = 0
    types = set()
    for fname in os.listdir(path):
        m = file_pat.match(fname)
        if m:
            l, s, t = m.groups()
            layers = max(int(l), layers)
            steps = max(int(s), steps)
            types.add(t)

    res = {
        'layers': layers,
        'steps': steps,
        'types': sorted(types)
    }
    return res

def attn_heatmap(attention_weights: Union[torch.Tensor | np.ndarray], title: str = "Attention Heatmap", show: bool = True):
    """ 
    Parse attention weights from a tensor or numpy array and plot the heatmap
    Args:
        attention_weights: torch tensor or numpy array, shape [..., query_seq_len, key_seq_len]
    """
    import matplotlib.pyplot as plt

    # attention_weights: numpy ndarray or torch tensor, shape [..., seq_len, seq_len]
    # Only plot the last two dimensions

    # If attention_weights is a torch tensor, convert to numpy
    if isinstance(attention_weights, torch.Tensor):
        attn = attention_weights.detach().cpu().numpy()
    else:
        attn = attention_weights

    # Squeeze batch/head dims if present (assume shape is [*, seq_len, seq_len])
    if attn.ndim > 2:
        attn = attn.reshape(-1, attn.shape[-2], attn.shape[-1])[0]

    plt.figure(figsize=(6, 5))
    # query positions on y-axis (rows), reversed (0 on the top)
    plt.imshow(attn, aspect='auto', cmap='viridis', origin='upper')
    plt.colorbar(label="Attention Weight")
    plt.xlabel("Key positions")
    plt.ylabel("Query positions")
    plt.title(title)
    plt.tight_layout()
    if show:
        plt.show()
    return plt.axes()
